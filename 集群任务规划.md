# 集群任务规划（CN）
##### 2025/10/17 持续更新中 

- 主要调研关于集群机器人的任务规划算法
- 拓展了部分多学科领域（经济，生物，材料）中的集群涌现现象，可供借鉴
- 本仓库公开，欢迎将你看好的论文加入到该仓库中。
  - 参考已有的格式来加入（论文名字为5号标题）
  - 当有论文有多个链接时，建议使用 arXiv 或 ResearchGate 或者其他的公开链接，方便所有人都轻松自由地阅读到论文

## 目录

- [研究背景与动机](#研究背景与动机)
  
- [文献综述 Overview](#文献综述-overview)
  - [综述 - 多机器人任务分配 Multi Robot Task Allocation](#综述---多机器人任务分配-multi-robot-task-allocation)
    - [A comprehensive taxonomy for multi-robot task allocation](#a-comprehensive-taxonomy-for-multi-robot-task-allocation)
    - [A Formal Analysis and Taxonomy of Task Allocation in MultiRobot Systems](#a-formal-analysis-and-taxonomy-of-task-allocation-in-multirobot-systems)
    - [A Survey on Large Language Model-Based Game Agents](#a-survey-on-large-language-model-based-game-agents)
    - [A Systematic Literature Review on Multi-Robot Task Allocation](#a-systematic-literature-review-on-multi-robot-task-allocation)
    - [Optimization techniques for Multi-Robot Task Allocation problems: Review on the state-of-the-art](#optimization-techniques-for-multi-robot-task-allocation-problems-review-on-the-state-of-the-art)
    - [Market Approaches to the Multi-Robot Task Allocation Problem: a Survey](#market-approaches-to-the-multi-robot-task-allocation-problem-a-survey)
    - [Multi-robot Task Allocation: A Review of the State-of-the-Art](#multi-robot-task-allocation-a-review-of-the-state-of-the-art)
  - [小结 失落群岛的寻宝任务 - 任务分配](#小结-失落群岛的寻宝任务---任务分配)
    - [场景设定：失落群岛的寻宝任务](#场景设定失落群岛的寻宝任务)
    - [案例一：无依赖 (No Dependencies - ND) 的寻宝规则](#案例一无依赖-no-dependencies---nd-的寻宝规则)
    - [案例二：计划内依赖 (In-Schedule Dependencies - ID) 的寻宝规则](#案例二计划内依赖-in-schedule-dependencies---id-的寻宝规则)
    - [案例三：跨计划依赖 (Cross-Schedule Dependencies - XD) 的寻宝规则](#案例三跨计划依赖-cross-schedule-dependencies---xd-的寻宝规则)
    - [案例四：复杂依赖 (Complex Dependencies - CD) 的寻宝规则](#案例四复杂依赖-complex-dependencies---cd-的寻宝规则)
  - [综述 - 多机器人任务规划 Task Planning](#综述---多机器人任务规划-task-planning)
    - [集群协同任务规划的形式逻辑方法: 综述与展望](#集群协同任务规划的形式逻辑方法-综述与展望)
    - [Market-Based Multi robot Coordination: A Survey and Analysis](#market-based-multi-robot-coordination-a-survey-and-analysis)
  - [小结 失落群岛的寻宝任务 - 任务规划](#小结-失落群岛的寻宝任务---任务规划)
    - [情景设定：失落群岛的协同寻宝 (XD 任务)](#情景设定失落群岛的协同寻宝-xd-任务)
    - [算法一：市场机制规划师 (The Economist Planner)](#算法一市场机制规划师-the-economist-planner)
    - [算法二：显式优化规划师 (The Logician Planner)](#算法二显式优化规划师-the-logician-planner)
    - [算法三：学习驱动规划师 (The Veteran Planner)](#算法三学习驱动规划师-the-veteran-planner)

- [算法论文](#算法论文)
  - [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](#air-ground-collaboration-for-language-specified-missions-in-unknown-environments)
  - [BeeCluster: Drone Orchestration via Predictive Optimization](#beecluster-drone-orchestration-via-predictive-optimization)
  - [APEX-MR: Multi-Robot Asynchronous Planning and Execution for Cooperative Assembly](#apex-mr-multi-robot-asynchronous-planning-and-execution-for-cooperative-assembly)
  - [CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory](#citynavagent-aerial-vision-and-language-navigation-with-hierarchical-semantic-planning-and-global-memory)
  - [Compositional Coordination for Multi-Robot Teams with Large Language Models](#compositional-coordination-for-multi-robot-teams-with-large-language-models)
  - [SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models](#smart-llm-smart-multi-agent-robot-task-planning-using-large-language-models)
  - [LiP-LLM: Integrating Linear Programming and Dependency Graph With Large Language Models for Multi-Robot Task Planning](#lip-llm-integrating-linear-programming-and-dependency-graph-with-large-language-models-for-multi-robot-task-planning)
  - [LLM-drone: aerial additive manufacturing with drones planned using large language models](#llm-drone-aerial-additive-manufacturing-with-drones-planned-using-large-language-models)
  - [Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments](#long-horizon-planning-for-multi-agent-robots-in-partially-observable-environments)
  - [Heterogeneous Multi-robot Task Allocation and Scheduling via Reinforcement Learning](#heterogeneous-multi-robot-task-allocation-and-scheduling-via-reinforcement-learning)
  - [RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning](#roboballet-planning-for-multi-robot-reaching-with-graph-neural-networks-and-reinforcement-learning)
  - [SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models](#swarmbrain-embodied-agent-for-real-time-strategy-game-starcraft-ii-via-large-language-models)
  - [VillagerAgent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in Minecraf](#villageragent-a-graph-based-multi-agent-framework-for-coordinating-complex-task-dependencies-in-minecraf)

- [拓展](#拓展)
  - [从蜂群到市场价格计算](#从蜂群到市场价格计算)
  - [Scaling Behavior in the Dynamics of an Economic Index](#scaling-behavior-in-the-dynamics-of-an-economic-index)

- [实验](#实验)



***

***


## 研究背景与动机

​	集群机器人在安防、能源、交通等领域的应用越来越广泛。以安防为例，边境巡检需要大范围长时间监控，城市巡检要应对复杂的空域环境和突发事件，建筑物内部的安防又需要处理各种室内复杂场景。这些任务的共同点是：需要多个机器人协同工作，既要覆盖整个目标区域，又要能实时发现和识别可疑目标。

​	实际应用中遇到的问题包括: 环境本身十分复杂，飞行安全难保证，夜间作业又有各种限制。其次是通信问题，信号干扰、延迟都会影响机器人之间的协同。最主要的困难再于, 现在的系统还是十分依赖人工，专业人员得盯着处理各种信息，效率很低。

​	从技术角度看，目前的AI功能比较单一，各个模块基本是各干各的。从硬件到算法、从单机到集群，缺少一套完整的解决方案。飞行安全、认知决策、多机协同这几块还没能真正整合起来。

​	往后看，几个趋势比较明显：空地一体化的异构资源整合会越来越重要，低空经济和AI产业的结合会更紧密，整个系统也在从人工操控慢慢往自主智能方向走。

​	我们的想法是把智能分成三层来看：

- **物理智能（Physical Intelligence）**  
  这层主要是机器人本体的基础能力，像目标检测、识别、避障、飞行控制这些，技术已经比较成熟了，可以直接拿来用。

- **数字智能（Digital Intelligence）**  
  这层是单个智能体的认知决策能力，包括任务理解、路径规划、状态推理、学习优化等。可以用强化学习、规划算法，也可以用大模型，核心是数据驱动的决策。

- **集群智能（Swarm Intelligence）** ← **我们的重点**  
  这层是多智能体协同产生的系统级智能，涉及任务分配、协同决策、通信协调、自组织行为等。目标是让多个智能体高效协作，做到1+1>2。

我们的研究主要聚焦在集群智能这一层，在已有的物理智能和数字智能基础上，实现多机器人的自主协同。



***

***


## 文献综述 Overview

### 综述 - 多机器人任务分配 Multi Robot Task Allocation



##### A comprehensive taxonomy for multi-robot task allocation

- https://journals.sagepub.com/doi/abs/10.1177/0278364913496484

- IJRR 

- 2013

- 主旨思想

  - Fig2以任务为中心，明确了Task Types，核心问题在讨论“这个任务能分解吗？有几种分解方法？”

  <img src="./assets/截屏2025-11-04 10.26.17.png" style="zoom: 100%;" />

  - Fig3根据任务分配和存在的调度关系，明确了Dependency Types，核心问题在讨论“执行这个任务会如何影响其他任务的执行？”。下图中的线不仅仅表示时间依赖的先后顺序，也表示资源消耗的依赖关系。

  <img src="./assets/截屏2025-11-04 10.34.39.png" style="zoom: 100%;" />

  - 本文搭建了机器人任务分配（MRTA）领域与经典数学优化模型之间的桥梁，将不同复杂度的机器人问题精确地映射到成熟的数学模型上，从而帮助研究者理解问题的内在难度并借鉴已有的求解方法。

  | iTax 类别 (iTax Category)                           | 类别描述 (机器人领域)                                        | 对应的经典数学/优化模型 (数学领域)                           | 问题复杂度           |
  | :-------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :------------------- |
  | **无依赖 (ND)**  `No Dependencies`                  | 任务效能完全独立，分配任务时不需考虑顺序或与其他任务的协同。一个机器人执行任务A的成本/收益与它是否执行任务B无关。 | **线性求和分配问题 (Linear Sum Assignment Problem - LSAP)**   *一个经典的“一对一”指派问题，例如，将N个工人分配给N个任务以实现总成本最低。* | **P**                |
  | **计划内依赖 (ID)**   `In-Schedule Dependencies`    | 任务效能依赖于**同一个机器人**任务计划中的其他任务。最典型的例子是路径规划，任务执行的顺序会影响总成本（路程）。 | **多旅行商问题 (m-TSP)** **车辆路径问题 (VRP)** **广义分配问题 (GAP)** **机器调度问题 (Machine Scheduling)** *这些模型的核心是为单个代理（机器人）找到一个最优的任务序列或集合。* | **NP-hard**          |
  | **跨计划依赖 (XD)**   `Cross-Schedule Dependencies` | 任务效能依赖于**不同机器人**的任务计划。这通常由机器人间的协同、约束或资源共享引起（例如，任务A必须在任务B之前完成，但它们由不同机器人执行）。 | **集合划分问题 (Set Partitioning Problem)** (用于无重叠的联盟形成) **集合覆盖问题 (Set Covering Problem)** (用于有重叠的联盟形成) **带旁路/同步约束的VRP/调度问题** *这些模型处理的是多个代理计划之间的耦合和约束。* | **NP-hard**          |
  | **复杂依赖 (CD)**   `Complex Dependencies`          | 任务本身存在**多种有效的分解方式**，系统必须在分配任务的同时决定采用哪种分解方案。这是任务分解、分配和调度三个问题的完全耦合。 | **(无直接对应的成熟模型)**  目前还没有一个广为人知的标准模型能够完全捕捉这类问题的复杂性。这代表了该领域的一个前沿研究方向。 | **NP-hard**   (至少) |
  

***



##### A Formal Analysis and Taxonomy of Task Allocation in MultiRobot Systems

- https://journals.sagepub.com/doi/10.1177/0278364904045564

- IJRR 

- 2004

  


***

##### A Survey on Large Language Model-Based Game Agents

- https://arxiv.org/pdf/2404.02039
- 2024.4



***

##### A Systematic Literature Review on Multi-Robot Task Allocation

- https://www.researchgate.net/profile/Umashankar-Subramaniam/publication/384920503_A_Systematic_Literature_Review_on_Multi-Robot_Task_Allocation/links/671229f5035917754c07914d/A-Systematic-Literature-Review-on-Multi-Robot-Task-Allocation.pdf
- ACM Computing Survey 
- 2024

***



##### Optimization techniques for Multi-Robot Task Allocation problems: Review on the state-of-the-art

- https://www.researchgate.net/profile/Hamza-Chakraa/publication/372587435_Optimization_techniques_for_Multi-Robot_Task_Allocation_problems_Review_on_the_state-of-the-art/links/67b5a6b48311ce680c69c5b9/Optimization-techniques-for-Multi-Robot-Task-Allocation-problems-Review-on-the-state-of-the-art.pdf
- Robot. Auton. Syst
- 2023.10

***



##### Market Approaches to the Multi-Robot Task Allocation Problem: a Survey

- https://link.springer.com/article/10.1007/s10846-022-01803-0
- J. Intell. Robot. Syst
- 2023.2

***



##### Multi-robot Task Allocation: A Review of the State-of-the-Art

- https://www.researchgate.net/profile/Ahmed-Elmogy/publication/277075091_Multi-robot_Task_Allocation_A_Review_of_the_State-of-the-Art/links/60257382a6fdcc37a81d3e1d/Multi-robot-Task-Allocation-A-Review-of-the-State-of-the-Art.pdf
- Cooperative Robots and Sensor Networks
- 2015

***



### 小结 失落群岛的寻宝任务 - 任务分配



#### 场景设定：失落群岛的寻宝任务

- **探险队**：我们有两名勇敢的机器人探险家，**R1** 和 **R2**。
- **大本营 (Base)**：所有探险的起点和终点，位于群岛的南端。
- **宝藏**：岛上有四个失落的宝藏，分别位于不同的地点：
  - **A**: 古代钱币 (在西边的瀑布后)
  - **B**: 宝石高脚杯 (在东边的火山坑旁)
  - **C**: 水晶头骨 (在西边的神庙里，离 A 很近)
  - **D**: 黄金罗盘 (在东边的沉船里，离 B 很近)
- **目标**：以最少的总耗能（即最短的总路程）将所有四个宝藏带回大本营。

<img src="./assets/Generated Image November 04, 2025 - 11_49AM.png" style="zoom: 100%;" />

#### 案例一：无依赖 (No Dependencies - ND) 的寻宝规则

在这个版本的任务中，我们有一条**非常重要的规则**。

📜 **核心规则**

> 每个机器人的货舱很小，一次只能携带一件宝藏。因此，每当机器人找到一件宝藏后，**必须立刻返回大本营 (Base)** 卸货，然后才能出发去寻找下一个宝藏。

1. **任务分配的思考方式**：
   现在，指挥中心需要决定由谁去拿哪个宝藏。让我们来分析一下寻找“古代钱币 (A)”的任务。
   - 如果让 **R1** 去拿，它的成本是固定的：**路程(Base → A → Base)**。
   - 如果让 **R2** 去拿，它的成本也是固定的：**路程(Base → A → Base)**。
2. **“无依赖”的体现**：
   **R1** 执行任务 A 的成本，与它将来是否要去拿任务 B、C 或 D **完全没有关系**。因为每次任务都是一次独立的“往返旅行”。同理，分配任务 A 给谁的决定，也完全不影响分配任务 B 给谁的决定。它们是四个**完全独立**的决策。
3. **如何解决问题**：
   解决这个问题非常简单。我们只需要对每一个宝藏进行一次计算：
   - **宝藏 A**：谁离 A 更近？让谁去。
   - **宝藏 B**：谁离 B 更近？让谁去。
   - **宝藏 C**：谁离 C 更近？让谁去。
   - **宝藏 D**：谁离 D 更近？让谁去。
     这就是一个**线性分配问题 (LSAP)**。
4. **最终计划**：
   - **R1 的任务列表**：[取回 A, 取回 C] （R1 执行 A 和 C 的顺序不影响总成本，因为都是独立往返）
   - **R2 的任务列表**：[取回 B, 取回 D]




#### 案例二：计划内依赖 (In-Schedule Dependencies - ID) 的寻宝规则

现在，我们给机器人**升级装备**，规则也随之改变。

📜 **核心规则**

> 每个机器人都装备了一个大容量的“探险背包”，可以**一次携带多件宝藏**。机器人可以规划一条连续的路线，访问多个藏宝点，最后再返回大本营。

1. **任务分配的思考方式**：
   这个规则的改变，让问题变得复杂得多。我们再来分析分配任务。假设我们初步决定让 **R1** 去拿“古代钱币 (A)”和“水晶头骨 (C)”。
   - **R1** 现在不需要跑两次往返了。它可以规划一条最优路线：**路程(Base → A → C → Base)**。
   - 现在，获取宝藏 C 的**边际成本** (marginal cost) 不再是 Base → C → Base 的距离，而仅仅是 A → C 的距离！
   
2. **“计划内依赖”的体现**：
   
   - 一个任务的成本（或效用）**依赖于同一个机器人计划内的其他任务**。
   - 获取宝藏 C 的成本，取决于 **R1** 的上一站是哪里。
   - 任务的**执行顺序**也变得至关重要。路线 Base → A → C → Base 和 Base → C → A → Base 的总路程可能是不同的。
   - 但是请注意，**R1** 的计划和 **R2** 的计划之间仍然是**相互独立**的。**R1** 怎么规划它的路线，并不会影响 **R2** 如何规划它的路线。
   
   这就是“计划内依赖”的核心！
   
3. **如何解决问题**：
   我们不能再简单地把最近的宝藏分配给机器人了。因为 A 和 C 靠得很近，把它们打包分配给同一个机器人可能会产生巨大的“协同效应”，即使其中一个宝藏离另一个机器人更近。
   我们需要解决一个更复杂的问题，类似于**多旅行商问题 (m-TSP)**：
   - 如何将这 4 个宝藏分成两组？
   - 对于每一组，如何规划出最短的寻宝路线（旅行商问题）？
   - 哪种分组方式能让两条路线的总长度之和最小？
   
4. **最终计划的样子**：
   最终的计划不再是简单的任务列表，而是两条**最优的路径（或调度）**：
   - **R1 的计划**：“从大本营出发，先去 A，然后去 C，最后返回大本营。”
   - **R2 的计划**：“从大本营出发，先去 B，然后去 D，最后返回大本营。”




#### 案例三：跨计划依赖 (Cross-Schedule Dependencies - XD) 的寻宝规则

寻宝任务变得更加危险和复杂，出现了需要机器人**协同配合**才能解决的机关。

📜 **核心规则**

> 1. **新宝藏**: 岛屿中央的山洞里发现了第五件宝藏——**“山之心” (E)**。
>2. **机关联动**: 山洞的石门 (E点) 是紧锁的。要打开它，必须有人在东边的火山坑 (B点) **拉下一个古老的杠杆**。这个杠杆一旦拉下，石门只会开启很短的一段时间。
> 3. **重量限制**: “山之心” (E) 太重了，**必须由两个机器人合力**才能搬运回大本营。

1. **任务分配的思考方式**：
   现在，机器人的计划不再是独立的了。让我们分析一下新情况：
   - **因果链条**: 任何一个机器人想要进入山洞 (E) 拿宝藏，都**依赖于**另一个机器人先去火山 (B) 拉下杠杆。这就形成了一个**跨越机器人计划的先后顺序约束 (Precedence Constraint)**：任务B的完成 必须先于 任务E的进入。
   - **强制合作**: 搬运“山之心”(E) 这个任务本身就是一个**多机器人任务 (Multi-Robot Task)**。它不能被分配给单个机器人，必须分配给一个由 R1 和 R2 组成的“联盟”。
2. **“跨计划依赖”的体现**：
   - **R1 的最优路线**现在直接受到 **R2 计划**的影响（反之亦然）。
   - 假设 **R2** 被分配了去拉杠杆 (任务B)。它不能随时去，它必须计算好时间，确保当它拉下杠杆时，**R1** 已经等在山洞门口了，否则就是白费力气。
   - 同时，在规划完各自前序任务后，**R1 和 R2 都必须在山洞 (E) 汇合**，一起执行搬运任务，然后再一起返回大本-营。
   - 在 **ID** 案例中，我们只需要分别优化两条独立的路线。但现在，这两条路线被各种依赖关系**“捆绑”**在了一起，必须作为一个**整体**来联合优化。
3. **如何解决问题**：
   问题变得非常棘手。指挥中心必须解决一个**带复杂约束的联合调度问题**。它需要同时回答：
   - 谁去拉杠杆(B)，谁先去洞口等(E)？
   - 他们各自前面的寻宝路线 (A, C, D) 应该如何规划，才能确保他们在正确的时间点执行 B 和 E 的联动任务？
   - 如何最小化整个团队的总耗时或总路程？
4. **最终计划的样子**：
   最终的计划是一份高度同步和协调的**团队行动方案**：
   - **R1 的计划**: “先去 C 拿头骨，然后前往 E 等待。等石门开启后，与 R2 一起搬运 E 回大本营。”
   - **R2 的计划**: “先去 D 拿罗盘，然后去 B 拉下杠杆，并立即赶往 E 与 R1 汇合，共同搬运 E 回大本营。”




#### 案例四：复杂依赖 (Complex Dependencies - CD) 的寻宝规则

在最终的挑战中，机器人团队发现，完成协同任务的方法不止一种，不同的方法会带来完全不同的依赖关系网络。

📜 **核心规则**

> 获取宝藏E的基本前提不变（需两人合力搬运），但关于如何打开山洞的石门，探险队在一本古老的日志中发现了**两种截然不同的协议**。团队必须根据情况**选择其中一种**来执行。
>
> **协议一：“杠杆接力” (Decomposition 1 - The Lever Protocol)**
> 
> - **描述**：这正是案例三（XD）中的方法。一个机器人前往B点拉下杠杆（子任务B），另一个机器人必须在石门开启的时间窗口内于E点等候并进入（子任务E-enter）。
> - **依赖类型**：**先后顺序依赖 (Precedence)**。这个协议对时间的要求是“先……后……”，允许一定的容错窗口。
> 
> **协议二：“符文共鸣” (Decomposition 2 - The Harmony Protocol)**
> 
> - **描述**：在岛屿西侧的A点和东侧的D点各有一个古老的符文石。如果两个机器人**在完全相同的时刻**用手触碰这两个符文石，山洞的石门会永久性地打开。
>- **依赖类型**：**精确同步依赖 (Synchronization)**。这个协议对时间的要求是“在同一时刻”，约束极为严格，没有容错。

1. **任务分配的思考方式**：
   现在，指挥中心面临的不再仅仅是“如何协同”，而是“应该选择哪一种协同方式”的战略抉择。这个选择会从根本上改变整个任务的约束结构。
   - **分解选择**: “打开石门”这个高阶任务，现在有了两种截然不同的子任务分解方案（“杠杆接力”方案 vs. “符文共鸣”方案）。
   - **战略依赖**: 最佳协议的选择并不是孤立的，它完全取决于分配给两个机器人的其他任务（A, B, C, D）所形成的最优路径。选择哪种协议，会反过来影响其他任务的最佳分配和执行顺序。
2. **“复杂依赖”的体现**：
   - **任务分解的不确定性**: 问题的核心在于，任务的内在结构（即应该执行哪些子任务）本身是未知的，是待求解的变量之一。
   - **分解与调度的深度耦合**: 无法在规划路径之前，独立地判断哪种协议更好。如果机器人的最优路径恰好能让它们自然地在同一时间点分别到达A和D，那么“符文共鸣”协议的成本就极低。反之，如果同步的代价是巨大的绕路，那么选择成本相对固定的“杠杆接力”协议可能更为明智。
   - 整个问题从求解一个**固定的依赖网络**（XD），升级为在一个**包含多种可选依赖网络的集合**中寻找最优解。
3. **如何解决问题**：
   问题变得极度复杂。指挥中心必须在一个包含了“协议选择”这个额外维度的、更庞大的空间里进行搜索。它需要同时回答：
   - 团队应该采用哪一种任务分解方案（“杠杆接力”还是“符文共鸣”）？
   - 在选定的方案下，分解出的子任务（如去A、B或D）应该分配给谁？
   - 每个机器人的完整最优路径和时间表是什么？
     这三个问题紧密耦合，必须通过评估不同分解方案下的全局最优解来协同求解。
4. **最终计划的样子**：
   最终的计划是一套包含了顶层战略决策的完整团队行动方案，它明确了选择哪条路走。
   - **可能的最终计划**: “最终决定：采用‘符文共鸣’协议。R1的路径为Base→C→A，R2的路径为Base→B→D，两者须在14:52分整同步触碰符文石。之后共同前往E点汇合，搬运宝藏E并返回大本营。”

***

***



### 综述 - 多机器人任务规划 Task Planning

​	上述的多机器人任务分配（MRTA）的核心挑战在于回答“谁、在何时、以何种方式、去做什么”这一系列问题。为了解决这个复杂决策问题，学术界和工业界发展出了多种算法范式，它们各有侧重，适用于不同的应用场景。我们可以将这些方法分为三大主流思想：**基于市场机制的博弈方法**、**基于显式优化的规划方法**，以及新兴的**基于学习驱动的决策方法**。

***



##### Market-Based Multi robot Coordination: A Survey and Analysis

- https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1677943
- Proceedings of IEEE
- 2003

- 主旨思想
  - 该算法思想的灵感源于**人类社会高效的市场经济**。其核心哲学是：
    - **机器人是自利的经济人 (Self-interested Agents)**: 每个机器人被设计为追求**个体利益最大化**的实体。这个“利益”通常表现为最大化虚拟收益或最小化自身成本。
    - **任务是可交易的商品 (Tasks as Commodities)**: 待完成的任务被视为有价值的“商品”或“合同”，可以在机器人之间进行交易。
    - **看不见的手**: 在一个设计良好的市场系统中，机器人之间为了追逐个人利益而进行的“交易”行为，会**自然地、同时地**导向整个团队（即系统）的宏观效率提升。

***

##### 集群协同任务规划的形式逻辑方法: 综述与展望

- https://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c250223

- 自动化学报

- 2025

- 本文侧重于无人集群基于形式逻辑的复杂任务规划方法

- 核心思想

  - **基于显式优化的规划方法 (Explicit Optimization & Planning Methods)**

    ​	这篇论文将此类方法视为**严谨、精确且可解释**的“白盒”范式，是保证任务规划正确性和安全性的基石。其核心思想是**将任务规划问题转化为一个精确的数学或逻辑模型，然后利用成熟的算法进行求解**。论文中主要将其分为以下分支：

    - **a) 约束优化问题 (Constraint Optimization)**
      - **核心理念**: 侧重于**数值优化**。
      - **实现方式**: 通过定义实数和整数变量，直接构建一个包含**目标函数**（如：最小化总完成时间、降低总能耗）和一系列**约束条件**（如：任务依赖、时间窗口、资源限制）的数学模型。
      - **典型应用**: 论文指出，这在多车路径规划、车间调度等问题中已有成熟的建模模板，常使用Gurobi、CPLEX等商业求解器。
      - **论文评价 (优势)**: 建模过程严谨，结果可解释性强。
      - **论文评价 (劣势)**: 存在“**维数灾难**”问题。随着机器人和任务数量的增加，变量维度呈组合式增长，导致求解时间剧增，难以满足大规模、动态场景下的实时性要求。

    - **b) 形式化任务规划 (Formal Task Planning)**
      - **核心理念**: 侧重于**符号推理**与**逻辑表达**，强调任务的结构和逻辑的正确性。
      - **实现方式 (论文中细分的两种语言)**:

        - **任务描述语言 (ADL)**:
          - **代表**: STRIPS, PDDL, HTN。
          - **方法**: 将世界描述为离散的**状态**，将机器人行为定义为带有**前置条件**和**执行效果**的**动作**。规划过程就是在状态空间中搜索一条从初始状态到目标状态的合法动作序列。
          - **论文评价**: 具备良好的可解释性和完备性，但难以处理包含连续变量、复杂时序关系和高度不确定性的场景。

        - **时序逻辑语言 (Temporal Logic)**:
          - **代表**: LTL (线性时序逻辑), STL (信号时序逻辑)。
          - **方法**: 使用逻辑算子（如Always, Eventually, Until）来精确、无歧义地描述复杂的任务需求，特别是那些涉及**时间顺序、持续性监控和安全边界**的任务。例如，“机器人A必须在机器人B之后才能进入区域X”或“仓库温度必须始终保持在30度以下”。
          - **论文评价 (优势)**: 任务描述准确完备，逻辑推理严谨，支持自动化的验证与策略生成，尤其适合处理安全攸关和强逻辑依赖的任务。
          - **论文评价 (劣势)**: 同样面临“**状态空间爆炸**”的挑战，在大规模系统和实时任务中计算开销巨大。

  - **基于学习驱动的决策方法 (Learning-based Decision Methods)**

    ​	论文将此类方法，特别是以大语言模型（LLM）为代表的新兴技术，视为一种**强大、灵活但缺乏严谨性**的“黑盒”范式。其核心思想是**摆脱对人工精确建模的依赖，通过学习数据中的模式来直接生成决策**。

    - **核心理念**: 侧重于**认知智能**和**端到端**的解决方案。

    - **实现方式**:

      - **传统深度学习 (Traditional Deep Learning)**:

        ​	不依赖符号或逻辑推理，而是通过与环境交互或模仿专家数据来直接学习决策策略的范式。

        - **深度强化学习 (Deep Reinforcement Learning - DRL)**:
          - **工作方式**: 通过“**试错学习**”来优化策略。智能体（机器人）在环境中执行动作，环境会根据动作的好坏给出一个**奖励(Reward)**或**惩罚(Penalty)**信号。智能体的目标是学习一个能最大化长期累积奖励的策略。

        - **模仿学习 (Imitation Learning - IL)**:
          - **工作方式**: 通过“**模仿专家**”来学习策略。它需要一个包含大量专家决策数据的“示范集”。学习算法的任务是找出一个策略，使其在相同状态下做出的决策与专家的决策尽可能一致。

      - **大语言模型 (LLM)**: 

        ​	利用LLM卓越的自然语言理解和泛化推理能力，直接将人类用自然语言描述的模糊任务指令，转化为可执行的计划或代码。论文将其定位为构建“**感知-理解-规划-执行**”决策架构的关键。

        - **论文评价 (优势)**: **极大地降低了任务建模的专业门槛**，增强了人机交互的自然性和系统的环境适应性。具备强大的**零样本/少样本**学习能力。
        - **论文评价 (劣势)**: 面临三大核心挑战：1) **准确性与可解释性瓶颈**（存在“幻觉”，决策过程是黑箱）；2) **缺乏形式化验证**（难以保证生成的计划100%正确或安全）；3) **多约束协同优化能力不足**。


***

***



### 小结 失落群岛的寻宝任务 - 任务规划

​	让我们继续刚才的 寻宝任务的案例, 我们选择**案例三：跨计划依赖 (Cross-Schedule Dependencies - XD)** 作为我们的标准问题。

​	让我们回顾一下这个问题的定义: 

#### **情景设定：失落群岛的协同寻宝 (XD 任务)**

- **探险队**: **R1**, **R2**
- **宝藏**: A, B, C, D, 以及需要协同获取的 **E (“山之心”)**
- **核心规则**:
  2. 机器人有大背包，可以规划连续路径 (ID 特征)。
  3. 打开 E 点的石门，必须先有机器人去 B 点拉杠杆 (先后顺序约束)。
  4. 搬运 E 点的宝藏，必须由 R1 和 R2 合力完成 (多机器人任务约束)。
- **总目标**: 找到一个全局最优的团队计划，使得完成所有五个寻宝任务的**总路程最短**。



这个问题足够丰富，因为它包含了：

- **计划内依赖 (ID)**：机器人需要规划自己的最优路径（A,C,D）。
- **跨计划依赖 (XD)**：机器人之间存在**先后顺序**（先B后E）和**强制合作**（合力搬运E）的约束。



#### **算法一：市场机制规划师 (The Economist Planner)**

这位规划师相信“看不见的手”，它不发号施令，而是建立一个市场，让机器人自己通过竞争和协商来找到解决方案。

📜 **核心思想**

> 通过**拍卖 (Auction)** 和 **协商 (Negotiation)**，将任务作为“合同”分配给成本最低的机器人或机器人联盟。高效的全局计划会从机器人追求个体利益最大化的“自私”行为中涌现出来。

⚙️ **工作流程**

指挥官（拍卖师）不会一次性解决所有问题，而是将复杂的协同任务分解成一系列的交易：

1. **第一轮：简单任务拍卖 (拍卖 A, C, D)**
   - 指挥官首先拍卖那些单个机器人就能完成的任务：“现在拍卖宝藏A, C, D的拾取权！”
   - R1和R2各自计算拾取这些宝藏的**组合成本**。例如，R1发现自己打包获取{A, C}的成本很低，于是对这个“任务包”提交了一个有竞争力的标价。R2则可能赢得了{D}。
   - **结果**: 简单的任务分配完成。假设 R1 赢得 {A, C}，R2 赢得 {D}。
2. **第二轮：复杂合同协商 (协商 B 和 E)**
   - 现在只剩下最复杂的协同任务：拉杠杆(B)并合力搬运(E)。这不能简单拍卖，指挥官发布了一个**“团队合同”**。
   - R1和R2必须进行**点对点协商 (Peer-to-Peer Negotiation)**，形成一个“竞标联盟”。它们共同探讨：
     - **方案1**: R1去拉杠杆(B)，R2先去E点等待。
     - **方案2**: R2去拉杠杆(B)，R1先去E点等待。
   - 它们会分别计算出这两种方案下，**各自需要付出的总成本**（包括完成自己已中标的{A,C}或{D}）。
   - 最终，它们会选择那个**团队总成本最低**的方案（例如方案2），并作为一个联盟“接受”这个合同。

✅ **最终计划**

计划是一系列**合同的集合**。它不是由中央一次性生成的，而是通过多轮市场交易**逐步形成**的。例如：

- **R1的合同**: [获取宝藏A, 获取宝藏C, 在特定时间前到达E点等待]。
- **R2的合同**: [获取宝藏D, 去B点拉杠杆, 立即前往E点与R1汇合]。



***



#### **算法二：显式优化规划师 (The Logician Planner)**

这位规划师是一个追求完美的数学家。它相信所有问题都可以被精确地建模，并且一定存在一个理论上的“最优解”。

📜 **核心思想**

> 将整个任务（包括所有机器人、地点、规则和约束）转化为一个巨大的**数学优化模型**。然后，利用强大的**求解器 (Solver)**，从天文数字般的可能性中，一次性计算出那个**全局最优**的解。

⚙️ **工作流程**

这位规划师会拿出纸和笔（实际上是计算机），进行严谨的建模：

1. **定义变量**: 将整个问题数字化。例如，x_i_j_t = 1 代表“机器人i在时间t访问了地点j”，否则为0。成千上万个这样的变量构成了问题的解空间。
2. **定义目标函数**: 写下一个明确的数学目标：Minimize(总路程)。总路程是所有机器人行驶路径长度的总和。
3. **翻译所有规则为数学约束**:
   - 每个宝藏必须被访问一次 -> 一系列求和等式。
   - 机器人不能同时在同一个地方 -> 一系列不等式。
   - 拉杠杆(B)必须在进入山洞(E)之前 -> 时间变量的不等式：t_B < t_E。
   - 必须合力搬运E -> 位置和时间变量的等式：position(R1, t) = E AND position(R2, t) = E 必须在某个时间t同时成立。
4. **求解**: 将这个包含成千上万个变量和约束的**混合整数线性规划 (MILP)** 模型，输入到像 Gurobi 或 CPLEX 这样的专业求解器中。求解器会进行数小时甚至数天的计算。

✅ **最终计划**

计划是一份**精确到毫秒和毫米的、全局最优的行动甘特图**。它详细规定了每个机器人在每个时间点的确切位置和动作，并**从数学上保证**这是所有可能方案中总路程最短的那一个。

------



#### **算法三：学习驱动规划师 (The Veteran Planner)**

这位规划师是一位身经百战的“老兵”。它不依赖复杂的数学或市场规则，而是依靠在无数次模拟训练中积累的“直觉”和“经验”。

📜 **核心思想**

> 通过**深度强化学习 (Deep Reinforcement Learning)**，在一个高仿真的虚拟环境中进行数百万次的“演习”。让AI智能体通过不断的**试错**，学习到一个从**当前局面**直接映射到**最佳团队动作**的决策网络（策略）。

⚙️ **工作流程**

这位规划师的“规划”工作在任务开始前就已经完成了：

1. **建造“训练场”**: 创造一个与失落群岛一模一样的模拟环境。
2. **海量演习**: 让一个AI大脑控制R1和R2，在模拟器里执行**数百万次**寻宝任务。
   - **奖励**: 成功拿到宝藏、高效协作，就给予高额“奖励分”。
   - **惩罚**: 违反规则（如先去了E）、发生碰撞、浪费能源，就给予严厉“惩罚分”。
3. **形成“直觉”**: AI的神经网络在海量的奖惩激励下，会逐渐学习到各种复杂局面下的最佳应对模式。它不知道什么是“先后顺序约束”，但它知道，如果先去E，总会得到一个很低的分数，于是它学会了要先派一个机器人去B。它学到的是**模式**，而非**规则**。
4. **实战部署**: 将这个训练有素的AI大脑，直接部署到真实的机器人团队中。

✅ **最终计划**

它**没有一个预先生成的静态计划文档**。它的“计划”是一个**动态的、反应式的决策策略**。在真实任务的每一秒：

- AI观察整个战场（所有单位的位置）。
- 基于它强大的“直觉”（神经网络），**立刻**为R1和R2同时输出下一步应该执行的最佳动作。这个计划是**实时生成、步步为营**的。



***

***



## 算法论文

​	上一节中, 我们分析了基本的任务规划方案可以分为3大类: **基于市场机制的博弈方法**、**基于显式优化的规划方法**，以及新兴的**基于学习驱动的决策方法**。这一节中, 我们将分析一些这3类方法的论文, 包括沉淀了多年的经典高引用论文, 以及最近几年的新论文. 

***



### **基于市场机制的博弈方法**



***



### 基于显式优化的规划方法



##### BeeCluster: Drone Orchestration via Predictive Optimization

- https://people.csail.mit.edu/songtao/BeeCluster.pdf
- 2020
- 

本文用了一个简单的例子, 引出了为什么需要Predictive Optimization

![image-20251114200737120](./集群任务规划.assets/image-20251114200737120.png)

​	**图1**的代码展示了一个简单的主动感知循环的算法, 这段代码是许多智能算法的核心，如**梯度下降/爬山算法、粒子群优化**等，常用于解决目标位置未知、需要通过环境反馈来逐步逼近和锁定的问题。



​	第一行表示3个初始的探测点A/B/C

​	第二行表示接下来的操作将会一直循环, 永不停止

​	第三行value=SenseAtLocations({A,B,C}) 代表感知/测量步骤. 程序调用一个SenseAtLocation的函数, 让系统(比如无人机)去A/B/C这3个地方进行感知或测量, 函数执行后, 会得到一个返回的量values, 如果是探测温度, 那么values可能就是这3个点的具体温度读数

​	第四行 A,B,C=Update(A,B,C,values) 是更新/决策步骤, 程序调用一个Update的函数, 利用刚刚测量到的values数值, 用某种算法, 计算出A/B/C下一轮应该去的位置



​	这个系统的循环关键再于"主动", 系统不是被动地接收数据, 而是进行一个不断迭代的闭环: **感知->决策->移动->再感知->再决策...**



​	我们可以为此设想一个例子, 比如说我们之前寻宝的例子, 在一个区域用金属探测仪寻找圣杯. 



​	初始化(第一行): 我们设定有3个机器人, 从A/B/C三个初始位置开始探测, 形成一个三角形

​	开始任务(第二行): 机器人集群开始执行任务

​	第一轮感知(第三行): 机器人移动到A/B/C三个点, 启动金属探测器测量. 它们返回了测量值values, 如{A点: 40%, B点: 20%, C点: 10%}

​	第一轮决策(第四行): 中央电脑上的Update算法分析了这些数据, 它发现A点数据更高. 于是根据这个信息, 算法判断出圣杯更可能在A点的方向, 于是它计算出了新的位置A'/B'/C', 让整个三角形探测编队朝着A点的方向移动了一段距离

​	进入下一轮循环... A/B/C的位置已经更新为了A'/B'/C', 无人机集群继续飞到新的位置A'/B'/C', 再次执行第3行的感知操作



​	**图2**中, 作者针对该算法举出了一个潜在的问题: 

​	作者假设了单个机器人的场景, 在每一次迭代中, A/B/C 3个位置的测量都需要同一个机器人完成, 那么在一轮任务中, 机器人的先后分配顺序可能是{A1->B1->C1} 或者 {A1->C1->B1}. 

​	如果仅仅看一轮的话, 这两个路线所需要的移动距离都是相同的, 但是如果我们考虑多次迭代, 两者就存在优劣之分. 

​	在{A1->B1->C1}这一轮结束后, 机器人要从C1移动到A2, 距离记作C1A2;  在{A1->C1->B1}结束后, 机器人要从B1移动到A2, 距离记作B1A2, 从图2可以看出, B1A2的距离明显更短, 机器人只需要一半的时间就可以开始下一次的任务. 

​	通过这个案例, 我们就能理解过去的算法存在一些短视的问题, 所以本文提出了Predictive Optimization(预测优化)的方法.

​	

​	除此之外, 作者还给出了5个Predictive Optimization的优势

- 利用空闲资源进行"预测性执行" (Speculative Execution)

​	在一个**顺序执行**的“感知-更新-再感知”循环任务中（例如，沿着新修的公路进行勘测），如果只有一台无人机，它必须完成“飞行->感知->计算”的完整循环后，才能开始下一次飞行。如果有多台无人机，传统的“反应式”系统因为在任何时刻只有一个任务请求，所以也只会派出一台无人机，导致其他无人机处于**闲置状态**，造成资源浪费。

​	预测性优化能够**预测**出这是一个迭代循环任务，并**推算出下一轮感知任务的大致位置**。当第一架无人机正在执行耗时的“感知”或“计算”任务时，BeeCluster会命令一架闲置的无人机**提前飞往**那个被预测出的未来任务地点“待命”。	

​	![image-20251114205109605](./集群任务规划.assets/image-20251114205109605.png)



-  **避免在“动态任务取消”中浪费资源 (Dynamic Request Cancellation)**

​	在某些探索任务中（例如，用高斯过程勘测Wi-Fi信号强度），任务之间存在**负相关**。测量A点的信号后，可能会因为信息增益足够，而**取消**对A点附近B点的测量任务。一个“反应式”系统可能会同时派出两架无人机分别飞往A和B，如果飞往B的无人机在途中任务被取消，那么这次飞行就**完全是浪费**。

​	预测性优化通过分析历史执行数据，能够**预测出任务被取消的概率**。避免在大概率取消的任务上浪费宝贵的飞行时间和电量

![image-20251114205536450](./集群任务规划.assets/image-20251114205536450.png)

- **通过预判实现更高效的全局路径 (Efficient Routing)**

  ​	在迭代执行的多点任务中（例如，每轮都要访问A, B, C三个动态更新的点），一个“短视”的调度器在规划当前轮次的路径时，只关心如何以最短路径走完当前这几个点。这可能导致它选择的路径**终点**，离**下一轮任务的起点**非常远，造成了两次任务之间**长距离的“折返跑”**。

   	预测性优化不仅考虑当前路径的长度，还会将“**路径终点要离下一轮起点近**”这个未来因素作为优化目标之一。这使得它能选择一个**全局最优**的路径，**消除了迭代之间的无效飞行**，提升了整体效率。



- **支持“细粒度任务复用”以提升无人机利用率 (Fine-Grained Multiplexing)**

  ​	传统的无人机API通常将一个任务（例如，“送货”）从头到尾绑定给一架无人机。一旦开始送货，这架无人机就必须完成“飞到A点取货 -> 飞到B点送货”的全部流程，中途不能被打断去执行其他任务，这叫做“粗粒度”绑定，导致无人机利用率低下。

  ​	预测性优化迭代的API允许开发者将任务定义为“**可中断的 (Interruptible)**”。当无人机在A点取到货后，在飞往B点之前，它是可以“暂停”送货任务的。如果此时出现一个优先级更高、顺路或耗时很短的新任务（比如“在附近的C点拍张照”），调度器可以**智能地将这个新任务插入**，让无人机先去C点拍照，再继续飞往B点送货。这种**“动作级别”的任务穿插**，极大地**提高了单架无人机的利用率**。



- **动机五：实现超越单机续航的“连续不间断操作” (Continuous Operation)**

  ​	对于需要长时间连续执行的任务（例如，持续追踪一个移动目标），单架无人机的电池续航是一个无法逾越的障碍。“反应式”系统只能等到第一架无人机电量耗尽报警时，才匆忙派出第二架去接替，这中间必然会产生**服务中断**。
​	预测性优化能够**预测**出追踪任务在未来的路径，并且实时监控当前无人机的剩余电量。
​	**带来的好处**: 在第一架无人机电量耗尽**之前**，系统会提前派出满电的第二架无人机，飞往被预测的未来交接点“**埋伏**”。当交接时刻到来时，第二架无人机已经在目标附近，可以实现**无缝接力 (seamless hand-off)**，从而保证了**任务的连续性**，突破了单机续航的物理限制。





![image-20251114210126782](./集群任务规划.assets/image-20251114210126782.png)

**核心设计理念:** 

- **通过 BeeCluster API 实现“灵活性”**

  ​	**应用开发者**（图左上角的用户）与BeeCluster框架交互的唯一接口。它提供了一套高级的编程指令（例如 newTask, act），让开发者可以专注于**应用本身的业务逻辑**（例如，“先去A点拍照，如果照片分析结果大于阈值，就再派一个任务去B点”），而无需关心底层的、繁琐的无人机管理细节（例如，具体派哪架无人机、如何规划路径、如何避免碰撞等）。

  ​	

- **将应用行为建模为“动态任务图 (DTG)”——让系统成为“行为记录员”**
  	BeeCluster框架在运行时，会像一个“记录员”一样，**实时监控**应用程序通过API发出的每一个指令，并将这些指令的执行流程、依赖关系和同步点，自动构建成一个名为**动态任务图 (Dynamic Task Graph - DTG)** 的数据结构。

  ​	DTG成为了应用程序**行为的“指纹”**。它将开发者编写的、对系统而言是“黑箱”的程序逻辑，**转化成了**一个系统可以理解、分析和存储的**结构化数据**。系统不再需要开发者去“解释”他的程序意图，而是通过“观察”程序的运行自己去**学习和理解**。

  

- **通过匹配DTG来预测未来——让系统成为“历史预言家”**

  ​	 BeeCluster会将每一次程序运行产生的DTG作为**“历史经验”**（在图中被称为 \**Application Profiling Data\**）存储起来。当一个新任务开始运行时，*预测器 (Predictor)* 模块会不断地将当前实时生成的DTG片段，与历史数据库中的DTG进行**图匹配 (Graph Matching)**

  ​	一旦找到一个高度相似的历史模式，系统就可以做出一个合理的推断：“既然过去发生了A、B、C之后，紧接着发生了D，那么现在也极有可能在C之后发生D”。这样，系统就获得了**预测未来任务的能力**。这个预测不是通用的，而是**针对特定应用、由其自身历史行为驱动的**，从而解决了“不同应用需要不同优化策略”的难题。



***

***



### 基于学习驱动的决策方法

​	这是近些年最热门的主流方向, 基本2025年前后发表的论文没有用到学习的内容, 都很难发表到好期刊上. 因此这部分的论文篇幅数量最多. 



##### An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue

- https://arxiv.org/pdf/2511.04042
- arxiv 
- 2025.11
- Main Idea



***



##### Air-Ground Collaboration for Language-Specified Missions in Unknown Environments

- https://arxiv.org/pdf/2505.09108
- IEEE TFR
- 2025.5

![image-20251114132703360](./集群任务规划.assets/image-20251114132703360.png)

![image-20251114150030910](./集群任务规划.assets/image-20251114150030910.png)

![image-20251114144556777](./集群任务规划.assets/image-20251114144556777.png)

图1的流程清晰地介绍了该论文的算法思想: 

1. **输入: 人类的语言指令**

   操作员给出一个高层的\高层的指令: "*There is a chemical spill. We need help collecting information!*"

2. **步骤1: 语义理解和推理**

   UGV内置的LLM驱动的规划器(名字为SPINE)接收到指令, 会自主推断出和任务相关的关键信息, 比如说 "化学品泄漏和什么东西有关?"它分析出 barrels(桶)和people等比较相关.

3. **步骤2: 任务分解和多机器人协同**

   SPINE意识到, 仅仅靠自己区寻找上一步的barrels和people的效率很低, 它需要更多的信息

   于是它主动把任务分解, 向UAV发出一个具体的指令: "*Can you look for [barrels, people]?*"

4. **步骤3: 基于感知的世界建模**

   UAV在空中飞行, 使用传感器和感知算法, 进行环境扫描. 并返回UGV一个轻量化\结构化的语义地图, 包括关键物体的位置和可通行的路径信息. 

5. **步骤4:路径规划** 

​	UGV的SPINE规划其接收到地图后, 开始进行路径规划, 前往目标物体.

6. **步骤5和步骤6: 验证和持续更新**

   UAV将继续持续更新地图

   UGV在执行的时候, 也会使用自己的传感器来验证UAV地图的准确性. 比如如果UAV发现某一条路实际上是被障碍物挡住的, 无法通行, 那么SPINE会从地图上删除这个错误的路径, 并重新规划(对应)

7. **步骤7: 反馈**

   当UGV发现桶的时候, 它会向操作员报告



- 个人疑问的点:

  UAV都已经发现了People和Barrels了, 为什么还要UGV再过去检测一次? 为什么不使用UAV直接搜索目标? 可以换一个更有说服力的例子, 体现两者视角互补的不可替代性

***





##### APEX-MR: Multi-Robot Asynchronous Planning and Execution for Cooperative Assembly

- https://arxiv.org/pdf/2503.15836
- RSS
- 2025.3

***

##### CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory

- https://arxiv.org/pdf/2505.05622



***



##### Compositional Coordination for Multi-Robot Teams with Large Language Models

- https://arxiv.org/pdf/2507.16068
- homepage：https://sites.google.com/view/lan-cb?pli=1



***



##### SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models

- https://arxiv.org/pdf/2309.10062
- IEEE IROS
- 2024.10



***

##### LiP-LLM: Integrating Linear Programming and Dependency Graph With Large Language Models for Multi-Robot Task Planning

- https://arxiv.org/pdf/2410.21040
- IEEE RAL
- 2025.2



***

##### LLM-drone: aerial additive manufacturing with drones planned using large language models

- https://link.springer.com/article/10.1007/s41693-025-00162-0
- Construction Robotics
- 2025

***

##### Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments

- https://proceedings.neurips.cc/paper_files/paper/2024/file/7d6e85e88495104442af94c98e899659-Paper-Conference.pdf
- NIPS
- 2024 

***



##### Heterogeneous Multi-robot Task Allocation and Scheduling via Reinforcement Learning

- https://ieeexplore.ieee.org/abstract/document/10854527
- IEEE RAL
- 2025.3



***

##### RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning

- https://arxiv.org/pdf/2509.05397
- Science Robotics
- 2025.9

***



##### SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models

- https://arxiv.org/pdf/2401.17749
- 2024.1



***

##### VillagerAgent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in Minecraf

- https://arxiv.org/pdf/2406.05720
- 2024.6

***

***



## 拓展



##### 从蜂群到市场价格计算

- https://www.youtube.com/watch?v=rb_Mh1vAUx0

##### Scaling Behavior in the Dynamics of an Economic Index

- https://www.researchgate.net/publication/243772362_Scaling_Behavior_in_the_Dynamics_of_an_Economic_Index



***

***



## 实验

- ConwayLife生命游戏：https://conwaylife.com/
- Lenia： https://github.com/Chakazul/Lenia
- Particle Life: https://particle-life.com/