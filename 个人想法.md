## 个人想法

​	以下主要基于过去的专业以及阅读到的论文作出的一些看法。

​	个人觉得，机器人数据的获取是当前具身智能领域面临的主要挑战之一。首先，这类数据并非互联网现有的数据，而需要在实际机器人上重新采样获取，这会耗费大量人力和时间。数据获取是一个问题，Mobile Aloha 提供了一种简便的采样平台，此外，也有一些研究尝试在仿真环境中获取模拟数据。其次，不同形态的机器人本体各异，且现实世界中的任务类型极为多样，这导致不同机器人之间和不同场景下采样的数据难以通用。因此，研究者们通常倾向于使用相同类型的机器人本体和任务场景，例如机械臂在家庭场景做家务或人形机器人行走，因为这类机器人已有大量的相关数据可供使用。如果希望尝试新形态的机器人，则需要自行收集或生成大量该形态机器人在实际工作中的数据，这进一步增加了科研投入时间和成本。这也是当今为什么说, 要产学研结合，要找落地场景, 因为有价值的项目一定有人愿意来做, 用的人多了，就自然而然产生大量数据, 进一步推动该领域的算法, 从而形成正反馈，这也是当的新范式。

​	目前喜欢走用机器人数据这条路的原因，是曾经做计算机视觉的人又开始研究机器人了。ImagNet和GPT的成功让这些研究人员认为，只要数据足够，就可以学习到策略。

​	但作为从物理学和自动化毕业的我来说，我还是对此持有中立态度。一方面传统的基于建模和优化的方法确实很难应对本体愈发复杂的机器狗和人形机器人之类的硬件载体；另一方面，现在的机器人任务也愈发的多样化，不像曾经那样有单一稳定的工况，人们期望机器人要处理生活中各种各样的任务。

​	当今解决上述第一个问题的主流方法是用强化学习，结合IsaacGym或者Genesis进行大规模并行训练，更快地采集更多数据，训练网络解决机器人的本体控制问题。解决第二个问题的主流方法是用模仿学习，采集足够多的图像和动作数据，训练和微调网络，再部署到机器人上，VLA的思路就是如此。

​	但强化学习和模仿学习存在的问题也很明显，可解释性非常差，我们也不知道为什么神经网络能识别当前的情况、知道接下来该做什么动作，其输出不像建模时代那样可通过一系列的公式推导出。这就存在着人工智能安全问题，当然，目前也有研究人员正在探索可信和可解释人工智能，或许能解决这些问题。
​	第二个是失去了一定的自然科学性，只有工程性。比如数学和物理学等是自然科学，是在探索世界的底层真理，基于此类自然理论，发展出来的工程学，如理论力学和自动控制原理等，被用来解释工程背后的原理。目前的具身智能，更多是靠着大量的数据和堆砌的算力，研究方法距离底层理论太远了，相比之下更像劳动力密集型研究而非技术密集型。

​	相比于图像识别等纯软件应用，落地到实际机器人上的具身算法提出了更高的要求，具身算法必须是100%可预测，100%成功率的。软件算法如果失败的话，严重些就是一个程序的崩溃，具身智能的算法的失败会带来实体破坏和人身危险，我很难想我的房间里放着一个带着机械臂的机器人，它在屋子里走来走去，有时候拿着刀做饭切菜，有时候又开关抽屉柜子找东西，万一它突然指令错误，会不会忽然用刀切别的东西，或者把抽屉翻的乱七八糟。所以具身算法的风险指数和造成的后果会比纯软件高的多。仅仅是识别MNIST里的数字，现在的网络都做不到100%正确，何况现在具身智能在很多任务上距离100%标准还有非常远的距离，只是看着非常厉害，但是实际上用起来不过是一些玩具。现在真正落地的机器人，比如工业机器人和一些酒店里的送货机器人，都是基于建模和优化的理论做出来的，因为这些方法是可预测的；应用于军事的机器狗、无人机等，还是需要由人工来远程遥控的，用人的意识来代替机器人的自身算法。

​	我觉得研究下去是有必要的，但是我担心的是资本能支持多久。这一波具身智能的算法爆发是因为LLM的发展顺便带起来的。但我觉得具身智能距离落地可能还有个十几年时间才可能落地。资本不会有耐心等待这么久，如果三四年后，具身智能还是无法落地商用，那个时候资本是不是会放弃这个赛道，导致相关的研究人员缺少资金，导致相关的研究崩塌。

​	我也为当今和我一样该领域的研究生感到危机，曾经强化学习很火，很多人去做，但现在我们都知道，强化学习的流程很简单了，不缺会强化学习的人，不会的人也只需要几周网课就可以上手了。现在的模型训练也必然像强化学习那样走向通俗化，变成一个基础技能。现在处于20年代初期，大模型岗位十分缺人，所以相关人才被抢夺，现在太多人一窝蜂聚集在这个赛道上，大部分做的工作就是洗数据、调模型参数，待几年后，这些技术的壁垒会越来越低，越来越多人都拥有同质的技能，将面临很严重的岗位挤兑和失业的危险。



​	联想到人类本身，我们对于脑科学的认识尚未明确。或许随着具身智能的发展，我们能逐渐揭开谜底。如果有一天现在的路子走通了，具身智能落地了，那么我们该去反思人类的智慧究竟是独一无二的，还是自然选择下的训练结果。